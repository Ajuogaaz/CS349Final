{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d21693ab-0e9c-4bc3-97e2-e848de41406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"R0KLAMJVMGF60UMF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a67a59c-28ff-425b-9c42-af5d92e9520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import json\n",
    "\n",
    "\n",
    "def save_dataset(symbol):\n",
    "    api_key = \"R0KLAMJVMGF60UMF\"\n",
    "\n",
    "    ts = TimeSeries(key=api_key, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol, outputsize='full')\n",
    "\n",
    "    data.to_csv(f'./{symbol}_daily.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62436ac1-181c-45b8-a3ef-3ce82a6ca153",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset('MSFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af09e346-8952-475b-9abf-a8f9a7b09f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "history_points = 50\n",
    "\n",
    "def csv_to_dataset(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data = data.drop('date', axis=1)\n",
    "    data = data.drop(0, axis=0)\n",
    "   \n",
    "\n",
    "    data_normaliser = preprocessing.MinMaxScaler()\n",
    "    data_normalised = data_normaliser.fit_transform(data)\n",
    "     # using the last {history_points} open high low close volume data points, predict the next open value\n",
    "    ohlcv_histories_normalised =      np.array([data_normalised[i  : i + history_points].copy() for i in range(len(data_normalised) - history_points)])\n",
    "    next_day_open_values_normalised = np.array([data_normalised[:,0][i + history_points].copy() for i in range(len(data_normalised) - history_points)])\n",
    "    next_day_open_values_normalised = np.expand_dims(next_day_open_values_normalised, -1)\n",
    "\n",
    "    next_day_open_values = np.array([data[:,0][i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    next_day_open_values = np.expand_dims(next_day_open_values_normalised, -1)\n",
    "\n",
    "    y_normaliser = preprocessing.MinMaxScaler()\n",
    "    y_normaliser.fit(np.expand_dims( next_day_open_values ))\n",
    "    \n",
    "    assert ohlcv_histories_normalised.shape[0] == next_day_open_values_normalised.shape[0]\n",
    "    return ohlcv_histories_normalised, next_day_open_values_normalised, next_day_open_values, y_normaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d30dc-c71f-4f7b-8729-8b67f8d263c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv_histories, next_day_open_values, unscaled_y, y_normaliser = csv_to_dataset('MSFT_daily.csv')\n",
    "\n",
    "test_split = 0.9 # the percent of data to be used for testing\n",
    "n = int(ohlcv_histories.shape[0] * test_split)\n",
    "\n",
    "# splitting the dataset up into train and test sets\n",
    "\n",
    "ohlcv_train = ohlcv_histories[:n]\n",
    "y_train = next_day_open_values[:n]\n",
    "\n",
    "ohlcv_test = ohlcv_histories[n:]\n",
    "y_test = next_day_open_values[n:]\n",
    "\n",
    "unscaled_y_test = unscaled_y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "361247ae-fa82-4ec3-8d24-fde1069c831a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/MachineLearning/Finals/venv/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a48a6d4acad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Finals/venv/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(4)\n",
    "\n",
    "lstm_input = Input(shape=(history_points, 5), name='lstm_input')\n",
    "x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "x = Dense(64, name='dense_0')(x)\n",
    "x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "x = Dense(1, name='dense_1')(x)\n",
    "output = Activation('linear', name='linear_output')(x)\n",
    "model = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5914f-2161-40d3-a0fa-23c0212e5f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adabf27-8539-41d9-8b8f-4ec27c6f5ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
